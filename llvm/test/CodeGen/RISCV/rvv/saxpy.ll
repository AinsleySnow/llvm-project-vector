; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+experimental-v < %s \
; RUN:    | FileCheck %s

; Function Attrs: noinline nounwind optnone
define dso_local void @saxpy_vec(i64 %n, float %a, float* %x, float* %y) #0 {
; CHECK-LABEL: saxpy_vec
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:  addi	sp, sp, -128
; CHECK-NEXT: 	.cfi_def_cfa_offset 128
; CHECK-NEXT: 	sd	ra, 120(sp)
; CHECK-NEXT: 	sd	s0, 112(sp)
; CHECK-NEXT: 	.cfi_offset ra, -8
; CHECK-NEXT: 	.cfi_offset s0, -16
; CHECK-NEXT: 	addi	s0, sp, 128
; CHECK-NEXT: 	.cfi_def_cfa s0, 0
; CHECK-NEXT: 	andi	sp, sp, -64
; CHECK-NEXT: 	csrr	a4, vlenb
; CHECK-NEXT: 	slli	a5, a4, 3
; CHECK-NEXT: 	sub	sp, sp, a5
; CHECK-NEXT: 	sd	sp, -72(s0)
; CHECK-NEXT: 	slli	a4, a4, 3
; CHECK-NEXT: 	sub	sp, sp, a4
; CHECK-NEXT: 	sd	sp, -80(s0)
; CHECK-NEXT: 	fmv.w.x	ft0, a1
; CHECK-NEXT: 	sd	a0, -24(s0)
; CHECK-NEXT: 	fsw	ft0, -28(s0)
; CHECK-NEXT: 	sd	a2, -40(s0)
; CHECK-NEXT: 	sd	a3, -48(s0)
; CHECK-NEXT: 	ld	a0, -72(s0)
; CHECK-NEXT: 	ld	a1, -80(s0)
; CHECK-NEXT: .LBB0_1:                                # %for.cond
; CHECK-NEXT:                                        # =>This Inner Loop Header: Depth=1
; CHECK-NEXT: 	ld	a2, -24(s0)
; CHECK-NEXT: 	vsetvli	a2, a2, e32,m8,tu,mu
; CHECK-NEXT: 	sd	a2, -56(s0)
; CHECK-NEXT: 	beqz	a2, .LBB0_3
; CHECK-NEXT: # %bb.2:                                # %for.body
; CHECK-NEXT:                                        #   in Loop: Header=BB0_1 Depth=1
; CHECK-NEXT: 	ld	a2, -40(s0)
; CHECK-NEXT: 	vsetvli	zero, zero, e32,m8,tu,mu
; CHECK-NEXT: 	vle32.v	v8, (a2)
; CHECK-NEXT: 	ld	a2, -56(s0)
; CHECK-NEXT: 	ld	a3, -40(s0)
; CHECK-NEXT: 	slli	a2, a2, 2
; CHECK-NEXT: 	ld	a4, -48(s0)
; CHECK-NEXT: 	add	a2, a3, a2
; CHECK-NEXT: 	vse32.v	v8, (a0)
; CHECK-NEXT: 	sd	a2, -40(s0)
; CHECK-NEXT: 	vle32.v	v8, (a4)
; CHECK-NEXT: 	flw	ft0, -28(s0)
; CHECK-NEXT: 	vle32.v	v16, (a0)
; CHECK-NEXT: 	ld	a2, -48(s0)
; CHECK-NEXT: 	vse32.v	v8, (a1)
; CHECK-NEXT: 	vfmacc.vf	v8, ft0, v16
; CHECK-NEXT: 	vse32.v	v8, (a1)
; CHECK-NEXT: 	vse32.v	v8, (a2)
; CHECK-NEXT: 	ld	a2, -56(s0)
; CHECK-NEXT: 	ld	a3, -48(s0)
; CHECK-NEXT: 	slli	a2, a2, 2
; CHECK-NEXT: 	add	a2, a3, a2
; CHECK-NEXT: 	sd	a2, -48(s0)
; CHECK-NEXT: 	ld	a2, -56(s0)
; CHECK-NEXT: 	ld	a3, -24(s0)
; CHECK-NEXT: 	sub	a2, a3, a2
; CHECK-NEXT: 	sd	a2, -24(s0)
; CHECK-NEXT: 	j	.LBB0_1
; CHECK-NEXT: .LBB0_3:                                # %for.end
; CHECK-NEXT: 	addi	sp, s0, -128
; CHECK-NEXT: 	ld	s0, 112(sp)
; CHECK-NEXT: 	ld	ra, 120(sp)
; CHECK-NEXT: 	addi	sp, sp, 128
; CHECK-NEXT: 	ret
entry:
  %n.addr = alloca i64, align 8
  %a.addr = alloca float, align 4
  %x.addr = alloca float*, align 8
  %y.addr = alloca float*, align 8
  %l = alloca i64, align 8
  %vx = alloca <vscale x 16 x float>, align 64
  %vy = alloca <vscale x 16 x float>, align 64
  store i64 %n, i64* %n.addr, align 8
  store float %a, float* %a.addr, align 4
  store float* %x, float** %x.addr, align 8
  store float* %y, float** %y.addr, align 8
  br label %for.cond

for.cond:                                         ; preds = %for.inc, %entry
  %0 = load i64, i64* %n.addr, align 8
  %1 = call i64 @llvm.riscv.vsetvl(i64 %0, i64 19)
  store i64 %1, i64* %l, align 8
  %cmp = icmp ugt i64 %1, 0
  br i1 %cmp, label %for.body, label %for.end

for.body:                                         ; preds = %for.cond
  %2 = load float*, float** %x.addr, align 8
  %3 = call <vscale x 16 x float> @llvm.riscv.vload.nxv16f32.p0f32(float* %2)
  store <vscale x 16 x float> %3, <vscale x 16 x float>* %vx, align 64
  %4 = load i64, i64* %l, align 8
  %5 = load float*, float** %x.addr, align 8
  %add.ptr = getelementptr inbounds float, float* %5, i64 %4
  store float* %add.ptr, float** %x.addr, align 8
  %6 = load float*, float** %y.addr, align 8
  %7 = call <vscale x 16 x float> @llvm.riscv.vload.nxv16f32.p0f32(float* %6)
  store <vscale x 16 x float> %7, <vscale x 16 x float>* %vy, align 64
  %8 = load <vscale x 16 x float>, <vscale x 16 x float>* %vy, align 64
  %9 = load float, float* %a.addr, align 4
  %10 = load <vscale x 16 x float>, <vscale x 16 x float>* %vx, align 64
  %11 = call <vscale x 16 x float> @llvm.riscv.vfmacc.vf.nxv16f32.f32(<vscale x 16 x float> %8, float %9, <vscale x 16 x float> %10)
  store <vscale x 16 x float> %11, <vscale x 16 x float>* %vy, align 64
  %12 = load float*, float** %y.addr, align 8
  %13 = load <vscale x 16 x float>, <vscale x 16 x float>* %vy, align 64
  call void @llvm.riscv.vstore.p0f32.nxv16f32(float* %12, <vscale x 16 x float> %13)
  %14 = load i64, i64* %l, align 8
  %15 = load float*, float** %y.addr, align 8
  %add.ptr1 = getelementptr inbounds float, float* %15, i64 %14
  store float* %add.ptr1, float** %y.addr, align 8
  br label %for.inc

for.inc:                                          ; preds = %for.body
  %16 = load i64, i64* %l, align 8
  %17 = load i64, i64* %n.addr, align 8
  %sub = sub i64 %17, %16
  store i64 %sub, i64* %n.addr, align 8
  br label %for.cond

for.end:                                          ; preds = %for.cond
  ret void
}

; Function Attrs: nounwind
declare i64 @llvm.riscv.vsetvl(i64, i64) #1

; Function Attrs: nounwind readonly
declare <vscale x 16 x float> @llvm.riscv.vload.nxv16f32.p0f32(float*) #2

; Function Attrs: nounwind readnone
declare <vscale x 16 x float> @llvm.riscv.vfmacc.vf.nxv16f32.f32(<vscale x 16 x float>, float, <vscale x 16 x float>) #3

; Function Attrs: nounwind writeonly
declare void @llvm.riscv.vstore.p0f32.nxv16f32(float*, <vscale x 16 x float>) #4